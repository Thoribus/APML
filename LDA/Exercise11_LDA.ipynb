{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes: ###\n",
    "\n",
    "- recommended lda package (https://pypi.org/project/lda/) uses collapsed gibs sampling\n",
    "\n",
    "- recommended gensim libary uses online variational inference\n",
    "\n",
    "- currently used scikit learn uses online variational inference\n",
    "\n",
    "We could maybe compare them, if there's time  \n",
    "\n",
    "- We use Reuters 21578 dataset, lda example uses subset of Reuter News\n",
    "\n",
    "- lda can assign the best topic to each document easily, scikit can do that aswell, TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling with latent Dirichlet allocation (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LDA model describes each topic in terms of a distribution over words, and each document as a distribution over topics. The problem setting is unsupervised in the sense that only the text in the documents is observed, and all other variables are latent and need to be inferred by the model.\n",
    "\n",
    "We will study the <a href=\"https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\"> Reuters-21578 </a> document collection, which contains news wire articles that appeared on Reuters in 1987.\n",
    "\n",
    "First we download and load the Reuters-21578 dataset using the *Natural Language Toolkit* (*NLTK*), which is a platform for handling natural language in Python. It provides access to several text corpora and functions for handling text data. We first download the Reuters-21578 data and import it into the list $articles$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#! sudo pip install nltk\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')\n",
    "articles=[]\n",
    "for doc_id in reuters.fileids():\n",
    "    articles.append(reuters.raw(doc_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $articles$ now contains 10788 documents in the form of strings.\n",
    "\n",
    "LDA is a bag-of-words model, meaning that the probabilistic process for a document does not take into account the order in which the words appear in a document. This means that for performing inference with the LDA model, it suffices to know which words appear in a document and how often each word appears. Therefore we will pass the document collection as a document word frequency matrix. The document word frequency matrix is a 2D array where rows represent documents and columns represent words and each cell counts how many times the specific word appears in a given document.\n",
    "\n",
    "The following code constructs a document word matrix $tf$, pruning the vocabulary of words such that it only contains words  that:\n",
    "\n",
    "* have latin characters and are of length 3 or more characters (token_pattern='[a-zA-Z]{3,}'),\n",
    "\n",
    "* are not english stop words, that is, frequent but uninformative words such as \"the\" or \"a\" (stop_words='english')\n",
    "\n",
    "* occur in at least 0.2% of all documents and at most 95% of all documents (max_df=0.95, min_df=0.002)\n",
    "\n",
    "and out of these using the 2000 most frequent words (max_features=2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf = CountVectorizer( token_pattern='[a-zA-Z]{3,}',max_df=0.95, min_df=0.002,max_features=2000,stop_words='english')\n",
    "articles_words = tf.fit_transform(articles)\n",
    "word_index = tf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing the Latent Dirichlet Allocation\n",
    "Using sklearn.decomposition.LatentDirichletAllocation, we trained a LDA model with $K=20$ topics on the document collection using the document word frequency matrix *articles_words*. You can find the function documentation <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\"> here</a>. The Dirichlet prior parameters $\\alpha$ and $\\eta$ are left at their default values of $1/K$. Multicore processing is turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(max_iter=100, n_components=20, n_jobs=-1,\n",
       "                          random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(max_iter=100, n_components=20, n_jobs=-1,\n",
       "                          random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(max_iter=100, n_components=20, n_jobs=-1,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=20, random_state=42, n_jobs=-1, max_iter=100)\n",
    "lda.fit(articles_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each Topic Discribed in 10 Words\n",
    "We described each topic by listing the 10 most probable words in that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: said, trade, foreign, government, exports, year, canadian, canada, south, imports\n",
      "Topic 2: said, analysts, year, growth, economic, government, market, economy, policy, budget\n",
      "Topic 3: oil, said, gas, prices, opec, mln, crude, bpd, energy, production\n",
      "Topic 4: said, union, dlrs, oil, spokesman, strike, crude, today, port, company\n",
      "Topic 5: said, department, credit, usair, court, pacific, twa, air, dlrs, southern\n",
      "Topic 6: bank, pct, rate, market, said, banks, rates, money, stg, today\n",
      "Topic 7: mln, tonnes, production, nil, year, stocks, exports, total, wheat, week\n",
      "Topic 8: said, shares, pct, stock, company, offer, share, dlrs, common, group\n",
      "Topic 9: dollar, said, yen, japan, west, currency, exchange, bank, german, baker\n",
      "Topic 10: dlrs, said, mln, year, quarter, company, earnings, share, sales, loss\n",
      "Topic 11: said, wheat, grain, soviet, usda, agriculture, corn, program, crop, farmers\n",
      "Topic 12: said, trade, japan, japanese, states, united, officials, reagan, house, gulf\n",
      "Topic 13: billion, dlrs, mln, reserves, fed, deficit, surplus, bank, said, loans\n",
      "Topic 14: cts, april, record, dividend, div, pay, prior, split, qtly, march\n",
      "Topic 15: mln, stg, billion, profit, net, group, tax, year, francs, profits\n",
      "Topic 16: said, company, corp, mln, dlrs, unit, sale, acquisition, sell, agreement\n",
      "Topic 17: said, sugar, tonnes, coffee, gold, export, traders, market, futures, brazil\n",
      "Topic 18: said, new, stock, cocoa, prices, meeting, international, agreement, buffer, producers\n",
      "Topic 19: mln, cts, net, loss, shr, dlrs, qtr, revs, profit, year\n",
      "Topic 20: pct, year, january, february, rose, said, rise, december, fell, compared\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "topic_word_distributions = lda.components_\n",
    "\n",
    "# Display the top 10 words for each topic\n",
    "for topic_idx, topic in enumerate(topic_word_distributions):\n",
    "    top_words_indices = topic.argsort()[:-11:-1]  # Get indices of top 10 words\n",
    "    top_words = [word_index[i] for i in top_words_indices]\n",
    "    \n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: \n",
    "From the inferred topic distributions, we can define the topic distance between two documents $d_1$, $d_2$ to be the Kullback-Leibler divergence between their topic distributions. Let $\\theta_{d_j}$ for $j \\in \\{1,2\\}$ be the parameter vector of the categorical topic distributions for documents $d_1$ and $d_2$, and let $\\theta_{d_j,i}$ denote their $i$-th component, that is, the probability for topic $i$. \n",
    "\n",
    "The Kullback-Leibler divergence is defined by\n",
    "\n",
    "$$KL(d_1,d_2) = \\sum_i{\\theta_{d_1,i}}\\log{\\frac{\\theta_{d_1,i}}{\\theta_{d_2,i}}}$$\n",
    "\n",
    "Implement a function *get_similar(doc_id,doc_topic_distribution)* that takes an integer *doc_id* representing the document index and a matrix that gives the distribution over topics for each document. The function should return a list that contains the indices of all documents in the collection ordered by their topic distance to *doc_id*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4:  \n",
    "\n",
    "Get the 10  documents that are most similar to the document with index 1 according to topic distance, and inspect manually whether they are all indeed covering similar content as this document.\n",
    "\n",
    "To get the distribution of topics over documents, use the LatentDirichletAllocation transform function. It takes the document word matrix and returns an un-normalized document topic distribution, so you have to normalize the matrix before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
