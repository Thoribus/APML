{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Sampling and MCMC\n",
    "\n",
    "\n",
    "## Rejection Sampling\n",
    "\n",
    "\n",
    "taken from [https://www.deep-teaching.org/notebooks/monte-carlo-simulation/sampling/exercise-sampling-rejection]\n",
    "\n",
    "Rejection sampling is a simple and straightforward algorihtm to generate samples for distributions, which a hard or impossible to sample from, using a second enclosing distribution. Using the enclosing function we will sample points and accept them as sample points for our desired distribution if they lie under the desired distribution curve and otherwise reject them. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=40\n",
    "sigma_square_1 = 4.0\n",
    "sigma_square_2 = 4.0\n",
    "mu_1, sigma_1 = -2.5,np.sqrt(sigma_square_1)\n",
    "mu_2, sigma_2 = 3.5,np.sqrt(sigma_square_1)\n",
    "prob_1 = 0.4\n",
    "\n",
    "rv_1 = norm(loc = mu_1, scale = sigma_1)\n",
    "rv_2 = norm(loc = mu_2, scale = sigma_2)\n",
    "x_ = np.arange(-14, 16, .1)\n",
    "\n",
    "p_green = lambda x: prob_1 * rv_1.pdf(x) + (1-prob_1) * rv_2.pdf(x)\n",
    "plt.plot(x_, p_green(x_) , \"g-\",label='$p(x)$')\n",
    "\n",
    "sigma_red,mu_red = 5. , 1.\n",
    "q_red = norm(loc = mu_red, scale = sigma_red)\n",
    "\n",
    "plt.plot(x_, q_red.pdf(x_) , \"r-\",label='$q(x)$')\n",
    "plt.legend()\n",
    "\n",
    "_ = plt.xlabel(\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sample from the green distribution $p$ above but unfortunately we have no way to do so. We will use the normal distribution $q$ to obtain a sample from $p$ using rejection sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Implement rejection sampling to get a sample from $p(x)$. \n",
    "\n",
    "\n",
    "Let's first hypothesize that we identified a value $M$ such that:\n",
    "$\n",
    "\\forall x \\in \\mathbb{R}, p(x) < M\\cdot q(x)\n",
    "$\n",
    "\n",
    "The algorithm for rejection sampling works as follow:\n",
    "\n",
    "- Sample a value $x$ from $q$\n",
    "- Draw an value $u$ using an uniform distribution $\\mathcal{U}(0,1)$ and compare $u$ to $\\frac{p(x) }{M\\cdot q(x)}$\n",
    "   - if $u < \\frac{p(x) }{M\\cdot q(x)}$ then accept the sample $x$\n",
    "   - if $u \\geq \\frac{p(x) }{M\\cdot q(x)}$ discard the sample.\n",
    "\n",
    "\n",
    "You can use an histogram to verify that your sample is indeed approaching the $p$ distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code can go here.\n",
    "\n",
    "##TODO: value M ?!\n",
    "def rejection_sampling(size):\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < size:\n",
    "        x = np.random.normal(mu_red, sigma_red)\n",
    "        u = np.random.uniform(0, 1)\n",
    "\n",
    "        acceptance_prob = p_green(x) / q_red.pdf(x)\n",
    "\n",
    "        if u < acceptance_prob:\n",
    "            samples.append(x)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "samples = rejection_sampling(size)\n",
    "plt.hist(samples, bins=20, density=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text decoding using Metropolis-Hasting\n",
    "\n",
    "In this exercise, we will apply the Metropolis-Hastings algorith, to the problem of decoding encoded by substitution, assuming we known the language of the text. \n",
    "\n",
    "We assume also that we can model this language using bigrams (an order 1 Markov chain on the letters). More formally, this language is written over the alphabet $\\Lambda$. For example, in English, $\\Lambda$ contains upper and lower case letters, punctuation marks, numbers, etc... \n",
    "\n",
    "The bigram model is given by $\\mu$ and $A$:\n",
    " - $\\mu$ is the stationnary distribution of single letters probabilities\n",
    " - $A$ is the transition matrix between letters: it gives for each letter the probability of the next letter. \n",
    " \n",
    "This model can easily be estimated from a large corpus of text. An encoding (or decoding) function by substitution is a bijective function $\\tau: \\Lambda \\rightarrow \\Lambda$. \n",
    "If $T′$ is a text, the encoded text $T = \\tau(T′)$ is obtained by replacing each letter $c$ of $T′$ by $\\tau(c)$.\n",
    "\n",
    "Our formal problem is thus: given and encoded text $T = (c_1, c_2, . . . . , c_{|T|})$ ($c_i \\in \\Lambda, \\forall i$), retrieve the initial decoded text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "\n",
    "1. How do you compute the likelihood of a sequence $T = (c_1, c_2, \\cdots, c_{|T|})$ using a bigram model of parameter $(\\mu, A)$? \n",
    "\n",
    "We note the likelihood this way: \n",
    "$\n",
    "L(T, \\mu, A) = P(c_1, c_2, \\ldots, c_{|T|}\\mid \\mu, A) \n",
    "$\n",
    "\n",
    "One way to find the code would be to use simple Monte Carlo and sample encoding functions, using a distribution proportional to the likelihood of the decoded text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How many encoding functions are there? Is it possible to sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cannot sample directly a encoding function we will use MCMC. This method only needs the to know sampling probabilities up to a factor, which is the case here.\n",
    "\n",
    "We will thus construct a Markov chain on the space of all decoded text, by sampling decoding functions $\\tau$. \n",
    "The Metropolis-Hastings method works as follow:\n",
    "- Choose an initial state $\\tau_0$ arbitrarily\n",
    "- Repeat $N$ times the following steps:\n",
    "    - Compute $\\tau$ from $\\tau_t$ by swapping exactly 2 letters $c_1$ and $c_2$ in the encoding (see below for details on the function) \n",
    "    - Accept the transition from $\\tau_t$ to $\\tau$ with the probability $\\alpha(\\tau_t, \\tau)$ :\n",
    "    $$\n",
    "    \\alpha(\\tau_t, \\tau) = \\min \\left( 1, \\frac{L(\\tau(T), \\mu, A)\\cdot M(\\tau_t,\\tau)} \n",
    "    {L(\\tau_t(T), \\mu, A)\\cdot M(\\tau,\\tau_t)}\\right)\n",
    "    $$\n",
    "This mean that ones draw $u \\sim \\mathcal{U}(0,1)$:\n",
    "        - if $u < \\alpha(\\tau_t, \\tau)$ then $\\tau_{t+1} = \\tau$\n",
    "        - otherwise $\\tau_{t+1} = \\tau_{t}$\n",
    "\n",
    "\n",
    "After a sufficient large number of of iterations, the decoding function is drawn according to the probabilities of the decoded text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Show that the log probability of acceptation can be written as:\n",
    "$$\n",
    "\\log \\alpha(\\tau_t, \\tau) = \\min(0, \\mu(\\tau(c_1)) ) +\n",
    "\\sum_{i=1}^{|T|} log A(\\tau(c_{i-1}), \\tau(c_{i})) \n",
    "- \\mu(\\tau_t(c_1)) \n",
    "- \\sum_{i=1}^{|T|} log A(\\tau_t(c_{i-1}), \\tau_t(c_{i})) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Project\n",
    "\n",
    "1) Download the file `countWar.pkl` which contains the template of bigrams learned from Tolstoy's novel _War and Peace_. This text was chosen because it contains more than 3 million characters, which is long enough to learn a representative model of a language. Also download the `secret.txt` message to decode (or this one `secret2.txt`, same message coded differently in case you would have problems with the first file).\n",
    "\n",
    "2) Execute the following code to load in python the bigram model and the secret message.\n",
    "\n",
    "   - The variable `count` is a dictionary: for each letter of the novel _War and Peace_, it provides its number of occurrences in the novel.\n",
    "   - The pattern of bigrams is described by the variables `mu` and `A`.\n",
    "   - The variable `mu` is a vector that contains the initial probability distribution over the letters.\n",
    "   - `A` is a matrix that gives the probabilities of a letter given another letter.\n",
    "   - `secret` is a variable containing the message to be decoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Open the dictionary\n",
    "with open(\"./countWar.pkl\", 'rb') as f:\n",
    "    (count, mu, A) = pkl.load(f, encoding='latin1')\n",
    "\n",
    "with open(\"./secret2.txt\", 'r') as f:\n",
    "    secret = f.read()[0:-1] # -1 to suppress the line break\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The decoding functions will be represented as a dictionary where the key and the stored value are both of type character (one letter is encoded/decoded into another letter). \n",
    "  \n",
    "Write a function \n",
    "  ``` swapF : (char,char) dict -> (char,char) dict ``` <br>\n",
    "which takes as argument a decoding function $\\tau_t$ and returns a new decoding function $\\tau$ constructed by swapping two letters $c_1$ and $c_2$ as described in the previous question:\n",
    " -  $\\tau(c)=\\tau_t(c)$ for any $c$ such as $c \\ne c_1$ and $c \\ne c_2$;\n",
    " -  $\\tau(c_1)=\\tau_t(c_2)$\n",
    " -  $\\tau(c_2)=\\tau_t(c_1)$. \n",
    "\n",
    "\n",
    "You can test your function with the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def swapF(taut):\n",
    "    tau = taut.copy()\n",
    "    \n",
    "    values = list(tau.values())\n",
    "    first_letter = random.choice(values)\n",
    "    second_letter = random.choice(values)\n",
    "    \n",
    "    while first_letter == second_letter:\n",
    "        second_letter = random.choice(values)\n",
    "\n",
    "    for i in tau.keys():\n",
    "        if tau[i] == first_letter:\n",
    "            tau[i] = second_letter\n",
    "        elif tau[i] == second_letter:\n",
    "            tau[i] = first_letter\n",
    "\n",
    "    return tau\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = {'a' : 'b', 'b' : 'c', 'c' : 'a', 'd' : 'd' }\n",
    "print(swapF(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Write a function `decrypt: string x (char,char) dict -> string` <br>\n",
    "which, given a `mess` string and a decoding function `tau`, returns the string obtained by decoding `mess` by `tau`.\n",
    "\n",
    "You can test your function with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt(mess, tau):\n",
    "    decoded_mess = \"\"\n",
    "    for letter in mess:\n",
    "        if letter in tau.keys():\n",
    "            decoded_mess += tau[letter]\n",
    "        else:\n",
    "            decoded_mess += letter\n",
    "    return decoded_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = {'a' : 'b', 'b' : 'c', 'c' : 'a', 'd' : 'd' }\n",
    "print(decrypt (\"aabcd\", tau ))\n",
    "print(decrypt ( \"dcba\", tau ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will produce the following output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ">>> decrypt (\"aabcd\", tau )\n",
    " bbcad\n",
    ">>> decrypt (\"dcba\", tau )\n",
    " dacb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Create a dictionary (a hash table) associating to each character its index in `mu` or `A`. The code is simply the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars2index = dict(zip(np.array(list(count.keys())), np.arange(len(count.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`chars2index['a']` simply gives access to the index corresponding to the letter `a` in `mu` or `A`\n",
    "\n",
    "If you prefer, you can also use the index file that has already been generated: `fileHash.pkl`\n",
    "\n",
    "6) Write a function `logLikelihood: string x float np.array x float np.2D-array x (char,int) dict-> string` which, given a `mess` message (string), the arrays `mu` and `A` created in question 2 from `pickle` and the previous dictionary `chars2index`, returns the log-likelihood of the `mess` message with respect to the big diagram model `(mu, A)`.\n",
    "\n",
    "You can test your function with the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logLikelihood( \"abcd\", mu, A, chars2index )\n",
    "logLikelihood( \"dcba\", mu, A, chars2index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikelihood(mess,mu,A,chars2index):\n",
    "\n",
    "    # Initialize the log-likelihood to the log-probability of the first character\n",
    "    log_likelihood = np.log(mu[chars2index[mess[0]]])\n",
    "\n",
    "    # Iterate over the rest of the characters in the message\n",
    "    for i in range(1, len(mess)):\n",
    "        # Get the indices of the current and previous characters\n",
    "        prev_index = chars2index[mess[i - 1]]\n",
    "        curr_index = chars2index[mess[i]]\n",
    "        \n",
    "        # Add the log-probability of the transition from the previous character to the current one\n",
    "        log_likelihood += np.log(A[prev_index, curr_index])\n",
    "\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logLikelihood( \"abcd\", mu, A, chars2index ))\n",
    "print(logLikelihood( \"dcba\", mu, A, chars2index ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which will produce the following output:\n",
    "```\n",
    " >>> logLikelihood( \"abcd\", mu, A, chars2index )\n",
    " -24.600258560804818\n",
    " >>> logLikelihood( \"dcba\", mu, A, chars2index )\n",
    " -26.274828997400395\n",
    "```\n",
    "\n",
    "7) Code the Metropolis-Hastings method seen in TD as a function called `MetropolisHastings(mess, mu, A, tau, N, chars2index)` using the `swapF`, `decrypt` and `logLikelihood` functions:\n",
    "  - The `mess` parameter is the coded message.\n",
    "  - Parameters `mu` and `A` represent the bigram model.\n",
    "  - The argument `tau` is the initial decoding function to start the Metropolis-Hastings algorithm.\n",
    "  - The argument `N` is the maximum number of iterations of the algorithm.\n",
    "  - The argument `chars2index` has already been seen. \n",
    "\n",
    "The method is a simple loop where we do the following steps:\n",
    "  - draw a new decoding function `tau'` by applying swapF with the current decoding function tau as parameter\n",
    "  - calculation of the log-likelihood of the decoded message thanks to `tau'`.\n",
    "  - draw to accept or not to accept the transition to `tau'` given the the ratio of likelihoods\n",
    "  - if the transition is accepted, save the decoded message with the highest likelihood. \n",
    "\n",
    "The function returns the most likely decoded message. You will display the log-likelihood each time it is improved and the decoded message is saved.\n",
    "\n",
    "the function returns the most likely decoded message. You will display the log-likelihood each time it is improved and the corresponding decoded message so that you can observe the evolution of the algorithm.\n",
    "\n",
    "In order to test your function, you can execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MetropolisHastings(mess, mu, A, tau, N, chars2index):\n",
    "    # Initialize variables to store the best decoded message and its likelihood\n",
    "    best_decoded_message = decrypt(mess, tau)\n",
    "    best_likelihood = logLikelihood(best_decoded_message, mu, A, chars2index)\n",
    "    current_tau = tau\n",
    "    # Main Metropolis-Hastings loop\n",
    "    for i in range(N):\n",
    "        # Draw a new decoding function tau' by applying swapF to the current decoding function tau\n",
    "        tau_prime = swapF(current_tau)\n",
    "        # Calculate the log-likelihood of the decoded message with tau'\n",
    "        message_candidate = decrypt(mess, tau_prime)\n",
    "        likelihood_prime = logLikelihood(message_candidate, mu, A,chars2index)\n",
    "\n",
    "        # Draw to accept or reject the transition to tau' based on the ratio of likelihoods\n",
    "        if np.random.uniform(0, 1) < np.exp(likelihood_prime - best_likelihood):\n",
    "            # If the transition is accepted, update the best decoded message and likelihood\n",
    "            best_decoded_message = mess\n",
    "            best_likelihood = likelihood_prime\n",
    "\n",
    "            # Display the log-likelihood each time it is improved\n",
    "            print(f\"Iteration {i + 1}/{N} - Log-Likelihood: {best_likelihood:.4f}\")\n",
    "            \n",
    "            current_tau = tau_prime\n",
    "            \n",
    "    return best_decoded_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identityTau(count):\n",
    "    tau = {}\n",
    "    for k in list(count.keys()):\n",
    "        tau[k] = k\n",
    "    return tau\n",
    "\n",
    "message = MetropolisHastings( secret, mu, A, identityTau(count), 10000, chars2index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: this (silly) code doesn't work with `secret.txt`, only with `secret2.txt`!\n",
    "\n",
    "7) To speed up the calculations, we'll start from a decoding function taking the frequencies of occurrence of the letters (i.e. the most frequent letter of the coded message will be decoded to the most frequent letter observed in Tolstoy's novel; then the second most frequent letter of the coded message will be decoded to the second most frequent letter observed in the novel; and so on...). You can use the following code to build such a decoding function, named here `tau_init`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: mu = proba of the init characters, not the stationary proba.\n",
    "# => find frequent characters = sort (count) !\n",
    "# stationary character distribution\n",
    "freqKeys = np.array(list(count.keys()))\n",
    "freqVal = np.array(list(count.values()))\n",
    "# character index: +freq => - freq in references\n",
    "rankFreq = (-freqVal).argsort()\n",
    "\n",
    "# secret message analysis: index of the most frequent => least frequent\n",
    "keys = np.array(list(set(secret))) # all characters of secret2\n",
    "rankSecret = np.argsort(-np.array([secret.count(c) for c in keys]))\n",
    "# ATTENTION: 37 keys in secret, 77 in general... \n",
    "# Only the most frequent characters of mu are encoded, \n",
    "# so much the worse for the others.\n",
    "# alignment of + freq in mu VS + freq in secret\n",
    "tau_init = dict((keys[rankSecret[i]], freqKeys[rankFreq[i]]) for i in range(len(rankSecret)))\n",
    "\n",
    "MetropolisHastings(secret, mu, A, tau_init, 50000, chars2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The message will normally be intelligible when you reach a log-likelihood of more than -3090. However, there are usually still some errors in the translation... At which level? \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
